{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "İşBankası_MachineLearningChallenge_#3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVAxdXUihfaI"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCP3znGtkDZO"
      },
      "source": [
        "%reset -f\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from xgboost import plot_importance\r\n",
        "from matplotlib.pyplot import figure\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings(\"ignore\")\r\n",
        "from collections import Counter\r\n",
        "import statistics \r\n",
        "from statistics import mode \r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.model_selection import cross_val_score\r\n",
        "from sklearn.preprocessing import OneHotEncoder\r\n",
        "from sklearn.compose import make_column_transformer\r\n",
        "from sklearn.pipeline import make_pipeline\r\n",
        "\r\n",
        "from xgboost import XGBClassifier\r\n",
        "from sklearn.metrics import roc_curve,auc\r\n",
        "from sklearn.model_selection import StratifiedKFold\r\n",
        "import seaborn as sns \r\n",
        "from sklearn.preprocessing import OneHotEncoder\r\n",
        "\r\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfBpW4FEkEwU"
      },
      "source": [
        "train = pd.DataFrame(pd.read_csv(\"/train.csv\"))\r\n",
        "test = pd.DataFrame(pd.read_csv(\"/test.csv\"))\r\n",
        "exp = pd.DataFrame(pd.read_csv(\"/monthly_expenditures.csv\"))\r\n",
        "exp3 = pd.DataFrame(pd.read_csv(\"/İŞML#3/expv2_b.csv\"))\r\n",
        "exp4 = pd.DataFrame(pd.read_csv(\"/İŞML#3/expv2_b2.csv\"))\r\n",
        "exp5 = pd.DataFrame(pd.read_csv(\"/İŞML#3/yuzde_v1.csv\"))\r\n",
        "\r\n",
        "nntr = pd.DataFrame(pd.read_csv(\"/İŞML#3/nn_train_150i_v3.csv\"))\r\n",
        "nnts = pd.DataFrame(pd.read_csv(\"/İŞML#3/nn_test_150i_v3.csv\"))\r\n",
        "\r\n",
        "def refresh_sub():\r\n",
        "    sub = pd.read_csv('/sample_submission.csv')\r\n",
        "    sub = pd.DataFrame(sub)\r\n",
        "    sub = sub.drop(columns = 'target')\r\n",
        "    return sub\r\n",
        "def submit(prediction, name):\r\n",
        "    sub = refresh_sub()\r\n",
        "    sub.insert(1, 'target',prediction)\r\n",
        "    sub.to_csv(name , index = False, header = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1KjRQdnptCd"
      },
      "source": [
        "*********\r\n",
        "DATA EXPLORATION\r\n",
        "********"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXNYfCX_pIo3"
      },
      "source": [
        "sg=exp.groupby(\"sektor\").sum()\r\n",
        "fig, ax = plt.subplots(figsize=(25,7))\r\n",
        "plt.bar(sg.index, sg.aylik_toplam_tutar)\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5dq-_3guKsk"
      },
      "source": [
        "train.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdhyANAHYPny"
      },
      "source": [
        "test.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mo7M7XEkqD1J"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(19,6))\r\n",
        "#train.yas.hist(bins=30)\r\n",
        "np.log(train.loc[train.kidem_suresi > 0][\"kidem_suresi\"] ).hist(bins=30)\r\n",
        "#train.egitim.hist()\r\n",
        "#train.is_durumu.hist()\r\n",
        "#train.meslek_grubu.hist()\r\n",
        "#train.target.hist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BDYlkWVsXCa"
      },
      "source": [
        "print(train.loc[train.target == 0].shape[0] / (train.shape[0]) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e91gqhqlk-y"
      },
      "source": [
        "X_train.loc[train.target == 0].yas.hist(bins=32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zd5mEjYplbsS"
      },
      "source": [
        "X_train.loc[train.target == 1].yas.hist(bins=32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOnQ0qZ4Am8-"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(30,13))\r\n",
        "plt.scatter(train.loc[train.target==0].yas, train.loc[train.target==0].yas - train.loc[train.target==0].kidem_suresi/12,c=\"b\" ,s = 2)\r\n",
        "plt.scatter(train.loc[train.target==1].yas, train.loc[train.target==1].yas - train.loc[train.target==1].kidem_suresi/12,c=\"R\", s = 2)\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7Irik6zDVa0"
      },
      "source": [
        "train.yas - train.kidem_suresi/52"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inzC3ri8eHOT"
      },
      "source": [
        "exp5.loc[exp5.musteri == \"ffffdfc0f2\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDSTCk8bfSDG"
      },
      "source": [
        "disp_sum.iloc[-1].sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtPNKXN9puuB"
      },
      "source": [
        "******\r\n",
        "DATA PREPROCESSING\r\n",
        "*******"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4GhKSe1Mz-1"
      },
      "source": [
        "#dont run\r\n",
        "if True:\r\n",
        "    train_1 = train.copy();  test_1 = test.copy()\r\n",
        "    train_1 = train_1.drop(columns = \"tarih\");  test_1 = test_1.drop(columns = \"tarih\")\r\n",
        "    def tri_imputer(t, df, f2i, f):\r\n",
        "        print(f2i, \" is being imputed...\" )\r\n",
        "        [f1, f2] =  [f[j] for j in range(2)]\r\n",
        "        empty_df = t.loc[t[f2i] != t[f2i] ]\r\n",
        "        for i in range(empty_df.shape[0]):\r\n",
        "            mus = empty_df.iloc[i].musteri;    f1val = empty_df.iloc[i][f1];    f2val = empty_df.iloc[i][f2]\r\n",
        "            if f1val == f1val:\r\n",
        "                if f2val == f2val:\r\n",
        "                    try: df.loc[df.musteri == mus, f2i] = df.loc[(df[f1] == f1val) & (df[f2]== f2val) & (df[f2i] == df[f2i])][f2i].mode()[0]\r\n",
        "                    except: \r\n",
        "                        if df.loc[df[f1] == f1val].shape[0] > df.loc[df[f2] == f2val].shape[0]:\r\n",
        "                            try: df.loc[df.musteri == mus, f2i] = df.loc[(df[f1] == f1val) & (df[f2]== df[f2]) & (df[f2i] == df[f2i])][f2i].mode()[0]\r\n",
        "                            except: print(\"skipped11\")\r\n",
        "                        else:\r\n",
        "                            try: df.loc[df.musteri == mus, f2i] = df.loc[(df[f1] == df[f1]) & (df[f2]== f2val) & (df[f2i] == df[f2i])][f2i].mode()[0]\r\n",
        "                            except: print(\"skipped12\")\r\n",
        "                else:\r\n",
        "                    try: df.loc[df.musteri == mus, f2i] = df.loc[(df[f1] == f1val) & (df[f2]== df[f2]) & (df[f2i] == df[f2i])][f2i].mode()[0]\r\n",
        "                    except: print(\"skipped2\")\r\n",
        "            else:\r\n",
        "                if f2val == f2val:\r\n",
        "                    try: df.loc[df.musteri == mus, f2i] = df.loc[(df[f1] == df[f1]) & (df[f2]== f2val) & (df[f2i] == df[f2i])][f2i].mode()[0]\r\n",
        "                    except: print(\"skipped3\")\r\n",
        "                else:\r\n",
        "                    try: df.loc[df.musteri == mus, f2i] = df.loc[(df[f1] == df[f1]) & (df[f2]== df[f2]) & (df[f2i] == df[f2i])][f2i].mode()[0]\r\n",
        "                    except: print(\"skipped4\")\r\n",
        "        return df\r\n",
        "    if True:\r\n",
        "        tri_list = [\"egitim\", \"is_durumu\", \"meslek_grubu\"]\r\n",
        "        for i in range(0,3):\r\n",
        "            feature_to_impute = tri_list[i]\r\n",
        "            supporting_features = tri_list[:i] + tri_list[i+1:]\r\n",
        "            train_1 = tri_imputer(train, train_1, feature_to_impute, supporting_features)\r\n",
        "            test_1 = tri_imputer(test, test_1, feature_to_impute, supporting_features)\r\n",
        "        train_1.loc[train.kidem_suresi < 0, \"kidem_suresi\"] = train_1.loc[train.kidem_suresi > 1, \"kidem_suresi\"].mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdW9MEruhSvl"
      },
      "source": [
        "train_1 = train.copy();  test_1 = test.copy()\r\n",
        "train_1 = train_1.drop(columns = \"tarih\");  test_1 = test_1.drop(columns = \"tarih\")\r\n",
        "def bi_imputer(t, df, f2i, f):\r\n",
        "    print(f2i, \"is being imputed...\" )\r\n",
        "    empty_df = t.loc[t[f2i] != t[f2i] ]\r\n",
        "    for i in range(empty_df.shape[0]):\r\n",
        "        mus = empty_df.iloc[i].musteri;    fval = empty_df.iloc[i][f]\r\n",
        "        if fval == fval:\r\n",
        "            df.loc[df.musteri == mus, f2i] = df.loc[(df[f] == fval) & (df[f2i] == df[f2i])][f2i].mode()[0]\r\n",
        "        else:\r\n",
        "            df.loc[df.musteri == mus, f2i] = df.loc[(df[f] == df[f]) & (df[f2i] == df[f2i])][f2i].mode()[0]\r\n",
        "    return df\r\n",
        "bi_list = [\"egitim\", \"is_durumu\"]\r\n",
        "for i in range(0,2):\r\n",
        "    train_1 = bi_imputer(train, train_1, bi_list[i], bi_list[1-i])\r\n",
        "    test_1  = bi_imputer(test,  test_1,  bi_list[i], bi_list[1-i])\r\n",
        "train_1.loc[train_1.kidem_suresi < 0, \"kidem_suresi\"] = train_1.loc[train.kidem_suresi >= 0, \"kidem_suresi\"].mean()\r\n",
        "test_1.loc[test_1.kidem_suresi < 0, \"kidem_suresi\"] = test_1.loc[train.kidem_suresi >= 0, \"kidem_suresi\"].mean()\r\n",
        "print(\"Imputation completed.\")\r\n",
        "totdf = train_1.append(test_1, ignore_index = True).drop([\"target\"], axis=\"columns\")\r\n",
        "curmg = totdf.meslek_grubu\r\n",
        "totdf= pd.get_dummies(totdf, columns = [\"egitim\", \"is_durumu\"] )\r\n",
        "mgtestx  = totdf.loc[ totdf.meslek_grubu.isna() ].drop([\"meslek_grubu\"], axis =\"columns\")\r\n",
        "mgtrainx = totdf.loc[~totdf.meslek_grubu.isna() ]\r\n",
        "mgtrainx2= pd.get_dummies(mgtrainx, columns = [\"meslek_grubu\"] )\r\n",
        "totdf= mgtrainx2.append(mgtestx, ignore_index = False)\r\n",
        "totdf = totdf.sort_index()\r\n",
        "#totdf[str(colname + \"_x\")] = totdf[str(colname + \"_x\")].fillna(totdf[str(colname + \"_y\")])\r\n",
        "#totdf = totdf.drop(str(colname + \"_y\"), axis = \"columns\")\r\n",
        "mgtrainx = mgtrainx.drop([\"meslek_grubu\", \"musteri\"], axis =\"columns\")\r\n",
        "mgtestx = mgtestx.drop([\"musteri\"], axis =\"columns\")\r\n",
        "#mgtrainx = pd.get_dummies(mgtrainx, columns = [\"egitim\", \"is_durumu\"])\r\n",
        "#mgtestx = pd.get_dummies(mgtestx, columns = [\"egitim\", \"is_durumu\"])\r\n",
        "totdf = totdf.set_index(\"musteri\")\r\n",
        "totdf[\"musteri\"]=totdf.index\r\n",
        "totdf[\"meslek_grubu\"] = list(curmg)\r\n",
        "customers_to_predict = totdf.loc[ totdf.meslek_grubu.isna() ].musteri\r\n",
        "#totdf = totdf.drop(columns= \"musteri\")\r\n",
        "for mg in totdf.meslek_grubu.unique():\r\n",
        "    rf = RandomForestClassifier(random_state = 0)\r\n",
        "    colname = \"meslek_grubu_\" + str(mg)\r\n",
        "    if colname != \"meslek_grubu_nan\":\r\n",
        "        print(colname, \"is being predicted.\")\r\n",
        "        mgtrainy = totdf.loc[~totdf.meslek_grubu.isna(), colname ]\r\n",
        "        rf.fit(mgtrainx, mgtrainy)\r\n",
        "        predictions = rf.predict_proba(mgtestx)\r\n",
        "        prediction_df = pd.DataFrame({ \"musteri\": customers_to_predict, colname: predictions[:,1]})\r\n",
        "        totdf = totdf.drop(columns= \"musteri\")\r\n",
        "        totdf = totdf.merge(prediction_df, how=\"left\", left_index=True, right_index=True, copy=False)\r\n",
        "        #totdf = totdf.merge(prediction_df, how=\"outer\")\r\n",
        "        #totdf[str(colname + \"_x\")] = totdf[str(colname + \"_x\")].fillna(totdf[str(colname + \"_y\")])\r\n",
        "        totdf[\"musteri\"] = totdf.index\r\n",
        "        totdf.loc[totdf.meslek_grubu != totdf.meslek_grubu, colname + \"_x\"] = totdf.loc[totdf.meslek_grubu != totdf.meslek_grubu][colname + \"_y\"]\r\n",
        "        totdf = totdf.drop(columns = colname + \"_y\").rename(columns = {colname + \"_x\": colname})\r\n",
        "        #totdf = totdf.append(prediction_df).sort_values(colname).drop_duplicates(subset = \"musteri\")\r\n",
        "totdf = totdf.drop(columns = \"meslek_grubu\")\r\n",
        "totdf.index = totdf.musteri\r\n",
        "train_1 = pd.merge(train_1, totdf.loc[:,\t\"meslek_grubu_070e3be3ae\":], left_on=\"musteri\", right_index=True, how = \"inner\")\r\n",
        "test_1  = pd.merge(test_1, totdf.loc[:,\t\"meslek_grubu_070e3be3ae\":], left_on=\"musteri\", right_index=True, how = \"inner\")\r\n",
        "train_1 = train_1.drop(columns = [\"musteri_x\",\"musteri_y\",  \"meslek_grubu\"])\r\n",
        "test_1  = test_1.drop(columns =  [\"musteri_x\",\"musteri_y\",  \"meslek_grubu\"])\r\n",
        "print(\"Predictions completed.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epKYsLe_eqA9"
      },
      "source": [
        "def correlation_heatmap(train, s):\r\n",
        "    correlations = train.corr()\r\n",
        "    fig, ax = plt.subplots(figsize=(s,s))\r\n",
        "    sns.heatmap(correlations, vmax=1.0, center=0, fmt='.2f', cmap=\"YlGnBu\",\r\n",
        "                square=True, linewidths=.5, annot=True, cbar_kws={\"shrink\": .70})\r\n",
        "    plt.show();\r\n",
        "    \r\n",
        "correlation_heatmap(train_1[train_1.columns], 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBDVdsUgIMr_"
      },
      "source": [
        "def month(s):    return str(s)[-4:-2]\r\n",
        "exp.tarih = exp.tarih.apply(month)\r\n",
        "for m in [\"0\" + str(i) for i in range(1,7)]:\r\n",
        "    def add_month(s):    return str(s)+\"_\" + m\r\n",
        "    subexp = exp.loc[exp.tarih == m]\r\n",
        "    subexp.sektor = subexp.sektor.apply(add_month)\r\n",
        "    expg = subexp.groupby(['musteri', \"sektor\"], as_index = False).sum()\r\n",
        "    if m == \"01\":\r\n",
        "        expv = expg.pivot_table(index = [\"musteri\"], columns=\"sektor\", values = [\"aylik_toplam_tutar\"], aggfunc=\"sum\", fill_value=0)\r\n",
        "    else:\r\n",
        "        expvnext = expg.pivot_table(index = [\"musteri\"], columns=\"sektor\", values = [\"aylik_toplam_tutar\"], aggfunc=\"sum\", fill_value=0)\r\n",
        "        expv = expv.merge(expvnext, on =\"musteri\", how=\"outer\")\r\n",
        "expv =  expv.merge(pd.DataFrame(exp.musteri.unique(), columns=[\"musteri\"]), on = \"musteri\", how=\"outer\")\r\n",
        "expv = expv.fillna(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFI7KJyQh9hT"
      },
      "source": [
        "correlation_heatmap(expv[expv.columns], 30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVaRYlvlUYpr"
      },
      "source": [
        "#dontrun\r\n",
        "if False:\r\n",
        "    sektorler = exp.sektor.unique()\r\n",
        "    sectors=[]\r\n",
        "    expvtemp=expv.copy()\r\n",
        "    for i in range(len(sektorler)):\r\n",
        "        sector =[col for col in expv.columns];    j=0\r\n",
        "        for k in range(len(sector)):\r\n",
        "            k-=j\r\n",
        "            if not str(sector[k][1]).startswith(sektorler[i]): sector.remove(sector[k]); j+=1\r\n",
        "        sectors.append(sector)\r\n",
        "        #print(sector)\r\n",
        "        colname= \"sum_exp_\" + str(sektorler[i])[0:7]\r\n",
        "        expvtemp[colname] = expvtemp[sector[0:12]].sum(axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8pH_Gu0CrlO"
      },
      "source": [
        "# weighting\r\n",
        "if True:\r\n",
        "    sektorler = exp.sektor.unique()\r\n",
        "    sectors=[]\r\n",
        "    expv2=expv.copy()\r\n",
        "    for i in range(len(sektorler)):\r\n",
        "        sector =[col for col in expv.columns];    j=0\r\n",
        "        for k in range(len(sector)):\r\n",
        "            k-=j\r\n",
        "            if not str(sector[k][1]).startswith(sektorler[i]): sector.remove(sector[k]); j+=1\r\n",
        "        sectors.append(sector)\r\n",
        "        #print(sector)\r\n",
        "        colname= \"sum_exp_\" + str(sektorler[i])[0:7] + \"_weighted\"\r\n",
        "        expv2[colname] = np.dot(expv2[sector[0:12]], [i+1 for i in range(6)])\r\n",
        "    expv2[\"all_weighted\"] = expv2.iloc[:,79: ].mean(axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4egqPXfmXGB"
      },
      "source": [
        "correlation_heatmap(expv[expv.columns], 32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2wj-ko2px05"
      },
      "source": [
        "*******\r\n",
        "FEATURE ENGINEERING\r\n",
        "********"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RE2bwK26GSqc"
      },
      "source": [
        "train_1[\"kayit_zamani\"] = train_1.yas - train_1.kidem_suresi/12\r\n",
        "test_1[\"kayit_zamani\"] = test_1.yas - test_1.kidem_suresi/12"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhURvDDz0A9x"
      },
      "source": [
        "subexp3 = exp3.loc[1:,[\"musteri\",\"degisim\" ,\"degisim.1\",\"degisim.2\",\"degisim.3\",\"degisim.4\",\"degisim.5\",\"degisim.6\",\"degisim.7\",\"degisim.8\",\"degisim.9\",\"degisim.10\",\"degisim.11\",\"degisim.12\"]]\r\n",
        "subexp3 = subexp3.astype({'degisim': 'float64', 'degisim.1': 'float64',  'degisim.2': 'float64', 'degisim.3': 'float64', 'degisim.4': 'float64', 'degisim.5': 'float64'\r\n",
        "                        , 'degisim.6': 'float64', 'degisim.7': 'float64', 'degisim.8': 'float64', 'degisim.9': 'float64', 'degisim.10': 'float64', 'degisim.11': 'float64', 'degisim.12': 'float64'  })\r\n",
        "try: expv3 = expv2.copy()\r\n",
        "except: expv3 = expv.copy()\r\n",
        "expv3 = expv3.merge(subexp3, on= \"musteri\", how=\"inner\")\r\n",
        "expv3 = expv3.drop(columns=(\"musteri\", \"\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oD2VXIEHmktU"
      },
      "source": [
        "correlation_heatmap(subexp3[subexp3.columns], 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCh6FfxhknMp"
      },
      "source": [
        "#dontrun\r\n",
        "if False:\r\n",
        "    def month(s):    return str(s)[-4:-2]\r\n",
        "    exp.tarih = exp.tarih.apply(month)\r\n",
        "    m_sum = exp.groupby([\"musteri\", \"tarih\"]).sum().pivot_table(index=\"musteri\", columns=\"tarih\", values=\"aylik_toplam_tutar\", aggfunc=\"sum\", fill_value=0)\r\n",
        "    #m_sum_trunc = m_sum[[\"01\", \"05\", \"06\"]]\r\n",
        "    expv3 = expv3.merge(m_sum, on=\"musteri\", how=\"inner\")\r\n",
        "    #expv3 = expv3.drop(columns= (\"musteri\", \"\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4bq5qiJnmsM"
      },
      "source": [
        "correlation_heatmap(m_sum[m_sum.columns], 12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSUnHhS4ohou"
      },
      "source": [
        "correlation_heatmap(expv3[expv3.columns], 32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60ULTHSR4Zls"
      },
      "source": [
        "from scipy.stats import linregress\r\n",
        "def trend(x):    return linregress([i for i in range(6)], x).slope\r\n",
        "disp_sum = exp.groupby([\"musteri\", \"tarih\", \"sektor\"]).sum().pivot_table(index=\"musteri\", columns=[\"tarih\", \"sektor\"], values=\"aylik_toplam_tutar\", aggfunc=\"sum\", fill_value=0)\r\n",
        "for i in range (0,13):\r\n",
        "    if i not in [99]:\r\n",
        "        st = str(i) + \"_Jan_Jun_%chg\"\r\n",
        "        sub_disp = disp_sum.iloc[:,0+i::13]\r\n",
        "        prc = (sub_disp.iloc[:,5]-sub_disp.iloc[:,0] ) / (sub_disp.iloc[:,0] + 1)\r\n",
        "        expv3[st] = list(prc)\r\n",
        "    ###\r\n",
        "    if i not in [99]:\r\n",
        "        st = str(i) + \"_trend\"\r\n",
        "        sub_disp = disp_sum.iloc[:,0+i::13]\r\n",
        "        slope = list(sub_disp.apply(trend, axis=1))\r\n",
        "        expv3[st] = slope\r\n",
        "    ##\r\n",
        "    if i not in [99]:\r\n",
        "        st = str(i) + \"_May_Jun_%chg\"\r\n",
        "        sub_disp = disp_sum.iloc[:,0+i::13]\r\n",
        "        prc = (sub_disp.iloc[:,5]-sub_disp.iloc[:,4] ) / (sub_disp.iloc[:,4] +1)\r\n",
        "        expv3[st] = list(prc)\r\n",
        "del sub_disp, prc, st, slope"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEbaomzY9Og6"
      },
      "source": [
        "correlation_heatmap(expv3[expv3.columns], 64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OBXjJep9gvA"
      },
      "source": [
        "correlation_heatmap(exp5[exp5.columns], 36)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WesoLsHmFOCe"
      },
      "source": [
        "#expv = expg.pivot_table(index = [\"musteri\"], columns=\"sektor\", values = [\"aylik_toplam_tutar\", \"islem_adedi\"], aggfunc=\"sum\", fill_value=0)\r\n",
        "#exp5 = pd.DataFrame(pd.read_csv(\"/content/drive/MyDrive/IS_ML/yuzde_v1.csv\"))\r\n",
        "exp5 = exp5.pivot_table(index = [\"musteri\"], columns=[\"sektor\", \"ay\"], values = \"yuzde\" , aggfunc=\"mean\", fill_value=0, dropna=False)\r\n",
        "exp5.musteri = exp5.index\r\n",
        "########exp5trunc = exp5.iloc[:,4::6].join(exp5.iloc[:,5::6])\r\n",
        "#exp5 = exp5.add_suffix(\"_%_of_month\")\r\n",
        "#######expv3 = expv3.merge(exp5trunc, on=\"musteri\", how=\"inner\")\r\n",
        "expv3 = expv3.merge(exp5, on=\"musteri\", how=\"inner\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCuZHco6Jl9K"
      },
      "source": [
        "correlation_heatmap(expv3[expv3.columns], 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZ1dxhG7CHnu"
      },
      "source": [
        "correlation_heatmap(exp6v[exp6v.columns], 36)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xB69p66FEmUO"
      },
      "source": [
        "exp6 = pd.DataFrame(pd.read_csv(\"/expb3-56.csv\"))\r\n",
        "exp6v = exp6.pivot_table(index = [\"musteri\"], columns=[\"sektor\"], values = \"islem_adedi_degisim_56\" , aggfunc=\"mean\", fill_value=0, dropna=False)\r\n",
        "exp6v = exp6v.add_suffix(\"_adet_may_june\")\r\n",
        "exp6v.musteri = exp6v.index\r\n",
        "expv3 = expv3.merge(exp6v, on=\"musteri\", how=\"inner\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HD0_uwccKpT2"
      },
      "source": [
        "correlation_heatmap(expv3[expv3.columns], 64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOMYwRwdTWzo"
      },
      "source": [
        "correlation_heatmap(exp7_bin[exp7_bin.columns], 40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNANOCBmLAdn"
      },
      "source": [
        "exp7_bin = pd.DataFrame(pd.read_csv(\"/tutar_binary.csv\"))\r\n",
        "exp7_bin = exp7_bin.pivot_table(index = [\"musteri\"], columns=[\"sektor\"], values = \"tutar_binary\" , aggfunc=\"mean\", fill_value=0, dropna=False)\r\n",
        "exp7_bin = exp7_bin.add_suffix(\"_may_jun_bin\")\r\n",
        "exp7_bin.musteri = exp7_bin.index\r\n",
        "expv3 = expv3.merge(exp7_bin, on=\"musteri\", how=\"inner\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lO9CYP_xSt8x"
      },
      "source": [
        "correlation_heatmap(expv3[expv3.columns], 80)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVBjv8vT8Ddd"
      },
      "source": [
        "# ALTERNATIVE\r\n",
        "expv3trunc = expv3.iloc[:,66:]\r\n",
        "expv3trunc[\"musteri\"] = expv3.musteri"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9w2TESJUruW"
      },
      "source": [
        "train2 = train_1.copy()\r\n",
        "test2 = test_1.copy()\r\n",
        "train2 = train2.merge(expv3, on=\"musteri\", how= \"inner\")\r\n",
        "test2 = test2.merge(expv3, on=\"musteri\", how= \"inner\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGGgZDRC6IY9"
      },
      "source": [
        "X_train = train2.drop(['target', \"musteri\"], axis='columns')\r\n",
        "y_train = train_1.target\r\n",
        "X_test = test2.drop([ \"musteri\"], axis='columns')\r\n",
        "\r\n",
        "X_train[\"NN_pred\"] = nntr[\"0\"]\r\n",
        "X_test[\"NN_pred\"] = nnts[\"target\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5dp_yDfpky2"
      },
      "source": [
        "*******\r\n",
        "PREDICTION MODELS BELOW\r\n",
        "*******"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "me0KPpW8yerB"
      },
      "source": [
        "counter = Counter(y_train)\r\n",
        "estimate = counter[0] / counter[1]\r\n",
        "print('Estimate: %.3f' % estimate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ps28U7HvmuWh"
      },
      "source": [
        "#X_train = train2.drop(['target', \"musteri\"], axis='columns')\r\n",
        "#y_train = train_1.target\r\n",
        "#column_trans = make_column_transformer((OneHotEncoder(handle_unknown = 'ignore'),['egitim', 'is_durumu']),remainder='passthrough')\r\n",
        "column_trans = make_column_transformer((OneHotEncoder(handle_unknown = 'ignore'),['egitim', 'is_durumu']),remainder='passthrough')\r\n",
        "for md in [3,4]:\r\n",
        "    for csbt in [1]:\r\n",
        "        for mcw in [1,2,3]:\r\n",
        "            for lr in [0.1]:\r\n",
        "                for n in [70]:\r\n",
        "                    for spw in [8]:\r\n",
        "                        for g in [0]:\r\n",
        "                            xgb = XGBClassifier(gamma = g, max_depth = md,scale_pos_weight = spw, n_estimators= n, colsample_bytree=csbt, min_child_weight = mcw, learning_rate = lr, eval_metric= \"auc\")\r\n",
        "                            pipe = make_pipeline(column_trans, xgb)\r\n",
        "                            print(\"mcwght: \",mcw ,\" l_r: \", lr  , \" n: \", n, \" spw: \", spw,\" csbt: \", csbt, \" max_d: \", md , \" gamma:\", g, \" AUC: \" ,cross_val_score(pipe, X_train, y_train, cv=5, scoring=\"roc_auc\").mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3b_qM0Jo0OUE"
      },
      "source": [
        "#X_test = test2.drop([ \"musteri\"], axis='columns')\r\n",
        "pipe.fit(X_train, y_train)\r\n",
        "pred_test = pipe.predict_proba(X_test)[:,1]\r\n",
        "submit(pred_test, r'/submissions/xgb_with_nn_v3_full.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7V669-scCK_"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(19,40))\r\n",
        "plt.barh(X_train.columns[0:150], xgb.feature_importances_[0:150])\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIFeQetUkzcs"
      },
      "source": [
        "pipe.fit(X_train, y_train)\r\n",
        "predtrain = pipe.predict_proba(X_train)\r\n",
        "#predtrain = xgb.predict_proba(X_train)\r\n",
        "from sklearn import metrics\r\n",
        "from sklearn.metrics import roc_auc_score\r\n",
        "# roc graph \r\n",
        "fpr, tpr, _ = metrics.roc_curve(y_train, predtrain[:,1])\r\n",
        "roc_auc = metrics.auc(fpr, tpr)\r\n",
        "fig, ax = plt.subplots(figsize=(8,8))\r\n",
        "plt.plot(fpr, tpr, color='darkorange',\r\n",
        "         lw=2, label='ROC AUC: %0.3f' % roc_auc)\r\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\r\n",
        "plt.xlim([0.0, 1.0])\r\n",
        "plt.ylim([0.0, 1.0])\r\n",
        "plt.xlabel('False Positive Rate')\r\n",
        "plt.ylabel('True Positive Rate')\r\n",
        "plt.title('Logistic Regression ROC')\r\n",
        "plt.legend(loc=\"lower right\")\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jj5q45F3dwuH"
      },
      "source": [
        "******\r\n",
        "MODEL VALIDATION\r\n",
        "*****"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mV101aUYvmH-"
      },
      "source": [
        "X_train.columns = ['{}_{}'.format(x[0], x[1]) for x in X_train.columns]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4zz6PPTd0sx"
      },
      "source": [
        "X_train.columns = ['{}_{}'.format(x[0], x[1]) for x in X_train.columns]\r\n",
        "numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns\r\n",
        "categorical_features = X_train.select_dtypes(include=['object']).columns\r\n",
        "#X_train = X_train.rename(columns='_'.join, inplace=True)\r\n",
        "\r\n",
        "X_trainval, X_testval, y_trainval, y_testval = train_test_split(X_train, y_train, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPBU_HyCgq15"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\r\n",
        "from sklearn.compose import ColumnTransformer\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "numeric_transformer = Pipeline(steps=[    ('scaler', \"passthrough\")])\r\n",
        "categorical_transformer = Pipeline(steps=[    ('one_hot', OneHotEncoder(handle_unknown = 'ignore'))])\r\n",
        "\r\n",
        "#numeric_transformer = \"passthrough\"\r\n",
        "preprocessor = ColumnTransformer(\r\n",
        "    transformers=[\r\n",
        "        ('num', numeric_transformer, numeric_features),\r\n",
        "        ('cat', categorical_transformer, categorical_features)\r\n",
        "    ])\r\n",
        "pipev = Pipeline(steps=[('preprocessor', preprocessor),\r\n",
        "                      ('classifier',  XGBClassifier(scale_pos_weight = 10, n_estimators= 85))])\r\n",
        "#X_trainval = X_trainval.replace([np.inf, -np.inf], np.nan)\r\n",
        "#X_testval = X_testval.replace([np.inf, -np.inf], np.nan)\r\n",
        "#X_trainval.fillna(0)\r\n",
        "#X_testval.fillna(0)\r\n",
        "model = pipev.fit(X_trainval, y_trainval)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZTaYfBdKPFI"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\r\n",
        "from sklearn.compose import ColumnTransformer\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "from sklearn.impute import SimpleImputer\r\n",
        "numeric_transformer = Pipeline(steps=[\r\n",
        "    ('imputer', SimpleImputer(strategy='median')),\r\n",
        "    ('scaler', StandardScaler())])\r\n",
        "categorical_transformer = Pipeline(steps=[\r\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\r\n",
        "    ('one_hot', OneHotEncoder(handle_unknown = 'ignore'))])\r\n",
        "preprocessor = ColumnTransformer(\r\n",
        "    transformers=[\r\n",
        "        ('num', numeric_transformer, numeric_features),\r\n",
        "        ('cat', categorical_transformer, categorical_features)\r\n",
        "    ])\r\n",
        "pipe = Pipeline(steps=[('preprocessor', preprocessor),\r\n",
        "                      ('classifier',  XGBClassifier(scale_pos_weight = 10, n_estimators= 85))])\r\n",
        "\r\n",
        "model = pipe.fit(X_trainval, y_trainval)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBLtGjdOmcup",
        "outputId": "4a6f0884-9574-4e52-d60e-52344b3b1566"
      },
      "source": [
        "from sklearn.metrics import classification_report\r\n",
        "target_names = y_testval.unique().astype(str)\r\n",
        "y_pred = model.predict(X_testval)\r\n",
        "print(classification_report(y_testval, y_pred, target_names=target_names))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.92      0.94     11525\n",
            "           1       0.14      0.32      0.19       475\n",
            "\n",
            "    accuracy                           0.89     12000\n",
            "   macro avg       0.55      0.62      0.57     12000\n",
            "weighted avg       0.94      0.89      0.91     12000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74kmM_ESsONP"
      },
      "source": [
        "onehot_columns = list(pipev.named_steps['preprocessor'].named_transformers_['cat'].named_steps['one_hot'].get_feature_names(input_features=categorical_features))\r\n",
        "numeric_features_list = list(numeric_features)\r\n",
        "numeric_features_list.extend(onehot_columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d77VqWsjsaeV"
      },
      "source": [
        "eli5.explain_weights(xgb, top=50, feature_names=numeric_features_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iyoFZWI1Kvf"
      },
      "source": [
        "Xp = preprocessor.fit_transform(X_trainval)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sj7zEH5PRUCP"
      },
      "source": [
        "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\r\n",
        "\r\n",
        "continuous_transformer = 'passthrough'\r\n",
        "categorical_columns = [\"egitim\", \"is_durumu\", \"meslek_grubu\"]\r\n",
        "\r\n",
        "continuous_columns = [col for col in X_train.columns if col not in categorical_columns]\r\n",
        "ct = ColumnTransformer(\r\n",
        "    [           ('categorical', categorical_transformer, categorical_columns),\r\n",
        "                ('continuous', continuous_transformer, continuous_columns),    ]\r\n",
        "    ,\r\n",
        "    sparse_threshold=0.,    n_jobs=-1)\r\n",
        "#pipe[0].get_feature_names()\r\n",
        "\r\n",
        "pipe2 = Pipeline(steps=[('preprocessor', ct),\r\n",
        "                      ('clf', xgb)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7wAGd06WKAj"
      },
      "source": [
        "pipe2.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3tfZQPUJw76"
      },
      "source": [
        "for name,step in pipe2.named_steps.items():\r\n",
        "    if hasattr(step, 'get_feature_names'):\r\n",
        "        print(step.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFTgQ3UPJPLH"
      },
      "source": [
        "pipe2['preprocessor'].transformers[1][1]['onehot']\\\r\n",
        "                   .get_feature_names(categorical_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzVCUZ1DTN8_"
      },
      "source": [
        "from xgboost import plot_importance\r\n",
        "pd.DataFrame(pipe.steps[1][1].feature_importances_.reshape(1, -1), columns=X_train.columns)\r\n",
        "#figure(num=None, figsize=(20, 14), dpi=80, facecolor='w', edgecolor='k')\r\n",
        "ax = plot_importance(xgb)\r\n",
        "fig = ax.figure\r\n",
        "fig.set_size_inches(20, 8)\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}